hyperparameters:
  batch_size: 4
  num_gpus: 2
  max_epochs: 10
  d_learning_rate: 1e-4     # 1e-4  used in UnSE paper
  g_learning_rate: 1e-4     # 1e-4  used in UnSE paper
  alpha_penalty: 10    # 10 used in UnSE paper
  alpha_fidelity: 10   # 10 used in UnSE paper
  n_critic: 10         # 10 used in UnSE paper

  dataset: "Finetune"   # "VCTK", "AudioSet", "Mix", "Speaker", "Finetune", "Unsuper50p" or "dummy" 
  num_speakers: x         # Used if dataset is "Speaker" or "FineTune"
  train_fraction: 0.01
  val_fraction: 0.02
  sisnr_loss: False    # False or integer alpha value (automatically set to 10 if dataset is "Finetune")

system:
  checkpointing: True
  continue_training: False
  ckpt_path: "models/test_unsuper50p_epoch=84.ckpt" # Or none
  num_workers: 20
  profiler: False  # False or "simple" or "advanced" or "pytorch"

wandb:
  name: ${now:%d/%m %H:%M:%S} Finetune
  log_all_scores: False # Causes slow training. Automatically turns on in val if dataset is not "VCTK"
  use_wandb: True
  entity: turtle_team
  project: bachelor2
  